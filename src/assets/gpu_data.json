[{"name": "V100", "vram": 17179869184, "membw": 966367641600, "citation": "https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf#page=15", "tdp": 300, "sms": 80, "cores_cuda": 5120, "cores_tensor": 640, "register_size": 20971520, "cache_l1": 10485760, "cache_l2": 6291456, "fp32_general": 15700000000000.0, "fp16": 120000000000000.0, "fp16_ignore_crippled": 120000000000000.0, "bf16": null, "bf16_ignore_crippled": null, "tf32": null, "int8": 240000000000000.0, "int4": 480000000000000.0, "fp8": null, "fp8_ignore_crippled": null, "fp6": null, "fp4": null, "crippled_fp32acc": false}, {"name": "T4", "vram": 17179869184, "membw": 343597383680, "citation": "https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/solutions/resources/documents1/Datasheet_NVIDIA_T4_Virtualization.pdf", "tdp": 70, "sms": 40, "cores_cuda": 2560, "cores_tensor": 320, "register_size": 10485760, "cache_l1": null, "cache_l2": null, "fp32_general": 8100000000000.0, "fp16": 65000000000000.0, "fp16_ignore_crippled": 65000000000000.0, "bf16": null, "bf16_ignore_crippled": null, "tf32": null, "int8": 130000000000000.0, "int4": 260000000000000.0, "fp8": null, "fp8_ignore_crippled": null, "fp6": null, "fp4": null, "crippled_fp32acc": false}, {"name": "2080ti", "vram": 11811160064, "membw": 343597383680, "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf#page=14", "tdp": 260, "sms": 68, "cores_cuda": 4352, "cores_tensor": 544, "register_size": 17825792, "cache_l1": null, "cache_l2": 5767168, "fp32_general": 14200000000000.0, "fp16": 56900000000000.0, "fp16_ignore_crippled": 113800000000000.0, "bf16": null, "bf16_ignore_crippled": null, "tf32": null, "int8": 227600000000000.0, "int4": 455200000000000.0, "fp8": null, "fp8_ignore_crippled": null, "fp6": null, "fp4": null, "crippled_fp32acc": true}, {"name": "Q6000", "vram": 25769803776, "membw": 721554505728, "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf#page=14", "tdp": 260, "sms": 72, "cores_cuda": 4608, "cores_tensor": 576, "register_size": 18874368, "cache_l1": null, "cache_l2": 6291456, "fp32_general": 16300000000000.0, "fp16": 130500000000000.0, "fp16_ignore_crippled": 130500000000000.0, "bf16": null, "bf16_ignore_crippled": null, "tf32": null, "int8": 261000000000000.0, "int4": 522000000000000.0, "fp8": null, "fp8_ignore_crippled": null, "fp6": null, "fp4": null, "crippled_fp32acc": false}, {"name": "Titan", "vram": 25769803776, "membw": 721554505728, "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf#page=44", "tdp": 280, "sms": 72, "cores_cuda": 4608, "cores_tensor": null, "register_size": 18874368, "cache_l1": 7077888, "cache_l2": 6291456, "fp32_general": 16300000000000.0, "fp16": 130500000000000.0, "fp16_ignore_crippled": 130500000000000.0, "bf16": null, "bf16_ignore_crippled": null, "tf32": null, "int8": 261000000000000.0, "int4": 522000000000000.0, "fp8": null, "fp8_ignore_crippled": null, "fp6": null, "fp4": null, "crippled_fp32acc": false}, {"name": "2070", "vram": 8589934592, "membw": 481036337152, "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf#page=44", "tdp": 215, "sms": 40, "cores_cuda": 2560, "cores_tensor": 320, "register_size": 10485760, "cache_l1": 3932160, "cache_l2": 4194304, "fp32_general": 9100000000000.0, "fp16": 36250000000000.0, "fp16_ignore_crippled": 72500000000000.0, "bf16": null, "bf16_ignore_crippled": null, "tf32": null, "int8": 145000000000000.0, "int4": 290000000000000.0, "fp8": null, "fp8_ignore_crippled": null, "fp6": null, "fp4": null, "crippled_fp32acc": true}, {"name": "3070", "vram": 8589934592, "membw": 481036337152, "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf#page=44", "tdp": 220, "sms": 46, "cores_cuda": 5888, "cores_tensor": 184, "register_size": 12058624, "cache_l1": 6029312, "cache_l2": 4194304, "fp32_general": 20300000000000.0, "fp16": 81300000000000.0, "fp16_ignore_crippled": 162600000000000.0, "bf16": 81300000000000.0, "bf16_ignore_crippled": 162600000000000.0, "tf32": 81300000000000.0, "int8": 325200000000000.0, "int4": 650400000000000.0, "fp8": null, "fp8_ignore_crippled": null, "fp6": null, "fp4": null, "crippled_fp32acc": true}, {"name": "3080", "vram": 10737418240, "membw": 816043786240, "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf#page=15", "tdp": 320, "sms": 68, "cores_cuda": 8704, "cores_tensor": 272, "register_size": 17825792, "cache_l1": 8912896, "cache_l2": 5242880, "fp32_general": 29800000000000.0, "fp16": 119000000000000.0, "fp16_ignore_crippled": 238000000000000.0, "bf16": 119000000000000.0, "bf16_ignore_crippled": 238000000000000.0, "tf32": 119000000000000.0, "int8": 476000000000000.0, "int4": 952000000000000.0, "fp8": null, "fp8_ignore_crippled": null, "fp6": null, "fp4": null, "crippled_fp32acc": true}, {"name": "3080ti", "vram": 12884901888, "membw": 979252543488, "citation": "https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf#page=32", "tdp": 350, "sms": 80, "cores_cuda": 10240, "cores_tensor": 320, "register_size": 20971520, "cache_l1": 10485760, "cache_l2": 6291456, "fp32_general": 34100000000000.0, "fp16": 68200000000000.0, "fp16_ignore_crippled": 136400000000000.0, "bf16": 68200000000000.0, "bf16_ignore_crippled": 136400000000000.0, "tf32": 68200000000000.0, "int8": 272800000000000.0, "int4": 545600000000000.0, "fp8": null, "fp8_ignore_crippled": null, "fp6": null, "fp4": null, "crippled_fp32acc": true}, {"name": "3090", "vram": 25769803776, "membw": 1005022347264, "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf#page=44", "tdp": 350, "sms": 82, "cores_cuda": 10496, "cores_tensor": 328, "register_size": 21495808, "cache_l1": 10747904, "cache_l2": 6291456, "fp32_general": 35600000000000.0, "fp16": 71000000000000.0, "fp16_ignore_crippled": 142000000000000.0, "bf16": 71000000000000.0, "bf16_ignore_crippled": 142000000000000.0, "tf32": 71000000000000.0, "int8": 284000000000000.0, "int4": 568000000000000.0, "fp8": null, "fp8_ignore_crippled": null, "fp6": null, "fp4": null, "crippled_fp32acc": true}, {"name": "3090ti", "vram": 25769803776, "membw": 1082331758592, "citation": "https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf#page=29", "tdp": 450, "sms": 84, "cores_cuda": 10752, "cores_tensor": 336, "register_size": 22020096, "cache_l1": 11010048, "cache_l2": 6291456, "fp32_general": 40000000000000.0, "fp16": 80000000000000.0, "fp16_ignore_crippled": 160000000000000.0, "bf16": 80000000000000.0, "bf16_ignore_crippled": 160000000000000.0, "tf32": 80000000000000.0, "int8": 320000000000000.0, "int4": 640000000000000.0, "fp8": null, "fp8_ignore_crippled": null, "fp6": null, "fp4": null, "crippled_fp32acc": true}, {"name": "A4000", "vram": 17179869184, "membw": 481036337152, "citation": "https://www.nvidia.com/content/dam/en-zz/Solutions/gtcs21/rtx-a4000/nvidia-rtx-a4000-datasheet.pdf", "tdp": 140, "sms": 48, "cores_cuda": 6144, "cores_tensor": 192, "register_size": null, "cache_l1": 8388608, "cache_l2": 4194304, "fp32_general": 19200000000000.0, "fp16": 76700000000000.0, "fp16_ignore_crippled": 76700000000000.0, "bf16": 76700000000000.0, "bf16_ignore_crippled": 76700000000000.0, "tf32": 38350000000000.0, "int8": 153400000000000.0, "int4": 306800000000000.0, "fp8": null, "fp8_ignore_crippled": null, "fp6": null, "fp4": null, "crippled_fp32acc": false}, {"name": "A4500", "vram": 21474836480, "membw": 687194767360, "citation": "https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx/nvidia-rtx-a4500-datasheet.pdf", "tdp": 200, "sms": 56, "cores_cuda": 7168, "cores_tensor": 224, "register_size": null, "cache_l1": 8388608, "cache_l2": 6291456, "fp32_general": 23700000000000.0, "fp16": 94600000000000.0, "fp16_ignore_crippled": 94600000000000.0, "bf16": 94600000000000.0, "bf16_ignore_crippled": 94600000000000.0, "tf32": 47300000000000.0, "int8": 189200000000000.0, "int4": 378400000000000.0, "fp8": null, "fp8_ignore_crippled": null, "fp6": null, "fp4": null, "crippled_fp32acc": false}, {"name": "A5000", "vram": 25769803776, "membw": 824633720832, "citation": "https://pnypartners.com/wp-content/uploads/nvidia-rtx-a5000-datasheet.pdf", "tdp": 230, "sms": 64, "cores_cuda": 8192, "cores_tensor": 256, "register_size": null, "cache_l1": 8388608, "cache_l2": 6291456, "fp32_general": 27800000000000.0, "fp16": 111100000000000.0, "fp16_ignore_crippled": 111100000000000.0, "bf16": 111100000000000.0, "bf16_ignore_crippled": 111100000000000.0, "tf32": 55550000000000.0, "int8": 222200000000000.0, "int4": 444400000000000.0, "fp8": null, "fp8_ignore_crippled": null, "fp6": null, "fp4": null, "crippled_fp32acc": false}, {"name": "A6000", "vram": 51539607552, "membw": 824633720832, "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf#page=15", "tdp": 300, "sms": 84, "cores_cuda": 10752, "cores_tensor": 336, "register_size": 22020096, "cache_l1": 11010048, "cache_l2": 6291456, "fp32_general": 38700000000000.0, "fp16": 154800000000000.0, "fp16_ignore_crippled": 154800000000000.0, "bf16": 154800000000000.0, "bf16_ignore_crippled": 154800000000000.0, "tf32": 77400000000000.0, "int8": 309600000000000.0, "int4": 619200000000000.0, "fp8": null, "fp8_ignore_crippled": null, "fp6": null, "fp4": null, "crippled_fp32acc": false}, {"name": "A10", "vram": 25769803776, "membw": 644245094400, "citation": "https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a10/pdf/datasheet-new/nvidia-a10-datasheet.pdf", "tdp": 150, "sms": 72, "cores_cuda": 9216, "cores_tensor": 288, "register_size": 22020096, "cache_l1": 9437184, "cache_l2": 6291456, "fp32_general": 31200000000000.0, "fp16": 125000000000000.0, "fp16_ignore_crippled": 125000000000000.0, "bf16": 125000000000000.0, "bf16_ignore_crippled": 125000000000000.0, "tf32": 62500000000000.0, "int8": 250000000000000.0, "int4": 500000000000000.0, "fp8": null, "fp8_ignore_crippled": null, "fp6": null, "fp4": null, "crippled_fp32acc": false}, {"name": "A40", "vram": 51539607552, "membw": 747324309504, "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf#page=15", "tdp": 300, "sms": 84, "cores_cuda": 10752, "cores_tensor": 336, "register_size": 22020096, "cache_l1": 11010048, "cache_l2": 6291456, "fp32_general": 37400000000000.0, "fp16": 149700000000000.0, "fp16_ignore_crippled": 149700000000000.0, "bf16": 149700000000000.0, "bf16_ignore_crippled": 149700000000000.0, "tf32": 74850000000000.0, "int8": 299400000000000.0, "int4": 598800000000000.0, "fp8": null, "fp8_ignore_crippled": null, "fp6": null, "fp4": null, "crippled_fp32acc": false}, {"name": "A100", "vram": 85899345920, "membw": 1669668536320, "citation": "https://resources.nvidia.com/en-us-genomics-ep/ampere-architecture-white-paper?xs=169656#page=1", "tdp": 400, "sms": 108, "cores_cuda": 6912, "cores_tensor": 432, "register_size": 28311552, "cache_l1": 11010048, "cache_l2": 41943040, "fp32_general": 19500000000000.0, "fp16": 312000000000000.0, "fp16_ignore_crippled": 312000000000000.0, "bf16": 312000000000000.0, "bf16_ignore_crippled": 312000000000000.0, "tf32": 156000000000000.0, "int8": 624000000000000.0, "int4": 1248000000000000.0, "fp8": null, "fp8_ignore_crippled": null, "fp6": null, "fp4": null, "crippled_fp32acc": false}, {"name": "L4", "vram": 25769803776, "membw": 322122547200, "citation": "https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf#page=39", "tdp": 72, "sms": 58, "cores_cuda": 7424, "cores_tensor": 232, "register_size": 15204352, "cache_l1": 7602176, "cache_l2": 50331648, "fp32_general": 30300000000000.0, "fp16": 121000000000000.0, "fp16_ignore_crippled": 121000000000000.0, "bf16": 121000000000000.0, "bf16_ignore_crippled": 121000000000000.0, "tf32": 60500000000000.0, "int8": 242000000000000.0, "int4": null, "fp8": 242000000000000.0, "fp8_ignore_crippled": 242000000000000.0, "fp6": null, "fp4": null, "crippled_fp32acc": false}, {"name": "L40", "vram": 51539607552, "membw": 927712935936, "citation": "https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf#page=36", "tdp": 300, "sms": 142, "cores_cuda": 18176, "cores_tensor": 568, "register_size": 37224448, "cache_l1": 18612224, "cache_l2": 100663296, "fp32_general": 90500000000000.0, "fp16": 181000000000000.0, "fp16_ignore_crippled": 181000000000000.0, "bf16": 181000000000000.0, "bf16_ignore_crippled": 181000000000000.0, "tf32": 90500000000000.0, "int8": 362000000000000.0, "int4": null, "fp8": 362000000000000.0, "fp8_ignore_crippled": 362000000000000.0, "fp6": null, "fp4": null, "crippled_fp32acc": false}, {"name": "L40S", "vram": 51539607552, "membw": 927712935936, "citation": "https://resources.nvidia.com/en-us-l40s/l40s-datasheet-28413", "tdp": 350, "sms": 142, "cores_cuda": 18176, "cores_tensor": 568, "register_size": 37224448, "cache_l1": 18612224, "cache_l2": 100663296, "fp32_general": 90500000000000.0, "fp16": 362050000000000.0, "fp16_ignore_crippled": 362050000000000.0, "bf16": 362050000000000.0, "bf16_ignore_crippled": 362050000000000.0, "tf32": 181025000000000.0, "int8": 724100000000000.0, "int4": null, "fp8": 724100000000000.0, "fp8_ignore_crippled": 724100000000000.0, "fp6": null, "fp4": null, "crippled_fp32acc": false}, {"name": "4000A", "vram": 21474836480, "membw": 300647710720, "citation": "https://www.nvidia.com/content/dam/en-zz/Solutions/rtx-4000-sff/proviz-rtx-4000-sff-ada-datasheet-2616456-web.pdf", "tdp": 130, "sms": 48, "cores_cuda": 6144, "cores_tensor": 192, "register_size": 12582912, "cache_l1": 6291456, "cache_l2": 50331648, "fp32_general": 19200000000000.0, "fp16": 76700000000000.0, "fp16_ignore_crippled": 76700000000000.0, "bf16": 76700000000000.0, "bf16_ignore_crippled": 76700000000000.0, "tf32": 38350000000000.0, "int8": 153400000000000.0, "int4": null, "fp8": 153400000000000.0, "fp8_ignore_crippled": 153400000000000.0, "fp6": null, "fp4": null, "crippled_fp32acc": false}, {"name": "6000A", "vram": 51539607552, "membw": 1030792151040, "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/technologies/NVIDIA-ADA-GPU-PROVIZ-Architecture-Whitepaper_1.1.pdf#page=28", "tdp": 300, "sms": 142, "cores_cuda": 18176, "cores_tensor": 568, "register_size": 37224448, "cache_l1": 18612224, "cache_l2": 100663296, "fp32_general": 91100000000000.0, "fp16": 364200000000000.0, "fp16_ignore_crippled": 364200000000000.0, "bf16": 364200000000000.0, "bf16_ignore_crippled": 364200000000000.0, "tf32": 182100000000000.0, "int8": 728400000000000.0, "int4": null, "fp8": 728400000000000.0, "fp8_ignore_crippled": 728400000000000.0, "fp6": null, "fp4": null, "crippled_fp32acc": false}, {"name": "4090", "vram": 25769803776, "membw": 1082331758592, "citation": "https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf#page=13", "tdp": 450, "sms": 128, "cores_cuda": 16384, "cores_tensor": 512, "register_size": 33554432, "cache_l1": 16777216, "cache_l2": 75497472, "fp32_general": 82600000000000.0, "fp16": 165150000000000.0, "fp16_ignore_crippled": 330300000000000.0, "bf16": 165150000000000.0, "bf16_ignore_crippled": 330300000000000.0, "tf32": 165150000000000.0, "int8": 660600000000000.0, "int4": null, "fp8": 330300000000000.0, "fp8_ignore_crippled": 660600000000000.0, "fp6": null, "fp4": null, "crippled_fp32acc": true}, {"name": "4080", "vram": 17179869184, "membw": 769657929728, "citation": "https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf#page=13", "tdp": 320, "sms": 76, "cores_cuda": 9728, "cores_tensor": 304, "register_size": 19922944, "cache_l1": 9961472, "cache_l2": 67108864, "fp32_general": 48700000000000.0, "fp16": 97450000000000.0, "fp16_ignore_crippled": 194900000000000.0, "bf16": 97450000000000.0, "bf16_ignore_crippled": 194900000000000.0, "tf32": 97450000000000.0, "int8": 389800000000000.0, "int4": null, "fp8": 194900000000000.0, "fp8_ignore_crippled": 389800000000000.0, "fp6": null, "fp4": null, "crippled_fp32acc": true}, {"name": "4070ti", "vram": 12884901888, "membw": 541380837376, "citation": "https://www.techpowerup.com/gpu-specs/geforce-rtx-4070-ti.c3950", "tdp": 285, "sms": 60, "cores_cuda": 7680, "cores_tensor": 240, "register_size": 15728640, "cache_l1": 7864320, "cache_l2": 50331648, "fp32_general": 40100000000000.0, "fp16": 77400000000000.0, "fp16_ignore_crippled": 154800000000000.0, "bf16": 77400000000000.0, "bf16_ignore_crippled": 154800000000000.0, "tf32": 77400000000000.0, "int8": 309600000000000.0, "int4": null, "fp8": 154800000000000.0, "fp8_ignore_crippled": 309600000000000.0, "fp6": null, "fp4": null, "crippled_fp32acc": true}, {"name": "H100-PCIe", "vram": 85899345920, "membw": 2199023255552, "citation": "https://resources.nvidia.com/en-us-tensor-core", "tdp": 350, "sms": 114, "cores_cuda": 14592, "cores_tensor": 456, "register_size": 33792, "cache_l1": null, "cache_l2": 52428800, "fp32_general": 66900000000000.0, "fp16": 756500000000000.0, "fp16_ignore_crippled": 756500000000000.0, "bf16": 756500000000000.0, "bf16_ignore_crippled": 756500000000000.0, "tf32": 378250000000000.0, "int8": 1513000000000000.0, "int4": null, "fp8": 1513000000000000.0, "fp8_ignore_crippled": 1513000000000000.0, "fp6": null, "fp4": null, "crippled_fp32acc": false}, {"name": "H100-SXM", "vram": 85899345920, "membw": 3599182594048, "citation": "https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet", "tdp": 700, "sms": 132, "cores_cuda": 16896, "cores_tensor": 528, "register_size": 33792, "cache_l1": null, "cache_l2": 52428800, "fp32_general": 66900000000000.0, "fp16": 989400000000000.0, "fp16_ignore_crippled": 989400000000000.0, "bf16": 989400000000000.0, "bf16_ignore_crippled": 989400000000000.0, "tf32": 494700000000000.0, "int8": 1978800000000000.0, "int4": null, "fp8": 1978800000000000.0, "fp8_ignore_crippled": 1978800000000000.0, "fp6": null, "fp4": null, "crippled_fp32acc": false}]