[{"s": "V100", "name_full": "???", "citation": "https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf#page=15", "tdp": 300, "sms": 80, "cores_cuda": 5120, "cores_tensor": 640, "register_size": 20971520, "cache_l1": 10485760, "cache_l2": 6291456, "vram": 17179869184, "membw": 966367641600, "fp32_general": 15700000000000.0, "fp16": 120000000000000.0, "has_bf16": false, "has_tf32": false, "has_int8": true, "has_int4": true, "has_fp8": false, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": false}, {"s": "T4", "name_full": "???", "citation": "https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/solutions/resources/documents1/Datasheet_NVIDIA_T4_Virtualization.pdf", "tdp": 70, "sms": 40, "cores_cuda": 2560, "cores_tensor": 320, "register_size": 10485760, "cache_l1": null, "cache_l2": null, "vram": 17179869184, "membw": 343597383680, "fp32_general": 8100000000000.0, "fp16": 65000000000000.0, "has_bf16": false, "has_tf32": false, "has_int8": true, "has_int4": true, "has_fp8": false, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": false}, {"s": "2080ti", "name_full": "???", "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf#page=14", "tdp": 260, "sms": 68, "cores_cuda": 4352, "cores_tensor": 544, "register_size": 17825792, "cache_l1": null, "cache_l2": 5767168, "vram": 11811160064, "membw": 343597383680, "fp32_general": 14200000000000.0, "fp16": 113800000000000.0, "has_bf16": false, "has_tf32": false, "has_int8": true, "has_int4": true, "has_fp8": false, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": true}, {"s": "Q6000", "name_full": "???", "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf#page=14", "tdp": 260, "sms": 72, "cores_cuda": 4608, "cores_tensor": 576, "register_size": 18874368, "cache_l1": null, "cache_l2": 6291456, "vram": 25769803776, "membw": 721554505728, "fp32_general": 16300000000000.0, "fp16": 130500000000000.0, "has_bf16": false, "has_tf32": false, "has_int8": true, "has_int4": true, "has_fp8": false, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": false}, {"s": "Titan", "name_full": "???", "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf#page=44", "tdp": 280, "sms": 72, "cores_cuda": 4608, "cores_tensor": null, "register_size": 18874368, "cache_l1": 7077888, "cache_l2": 6291456, "vram": 25769803776, "membw": 721554505728, "fp32_general": 16300000000000.0, "fp16": 130500000000000.0, "has_bf16": false, "has_tf32": false, "has_int8": true, "has_int4": true, "has_fp8": false, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": false}, {"s": "2070", "name_full": "???", "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf#page=44", "tdp": 215, "sms": 40, "cores_cuda": 2560, "cores_tensor": 320, "register_size": 10485760, "cache_l1": 3932160, "cache_l2": 4194304, "vram": 8589934592, "membw": 481036337152, "fp32_general": 9100000000000.0, "fp16": 72500000000000.0, "has_bf16": false, "has_tf32": false, "has_int8": true, "has_int4": true, "has_fp8": false, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": true}, {"s": "3070", "name_full": "???", "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf#page=44", "tdp": 220, "sms": 46, "cores_cuda": 5888, "cores_tensor": 184, "register_size": 12058624, "cache_l1": 6029312, "cache_l2": 4194304, "vram": 8589934592, "membw": 481036337152, "fp32_general": 20300000000000.0, "fp16": 162600000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": true, "has_fp8": false, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": true}, {"s": "3080", "name_full": "???", "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf#page=15", "tdp": 320, "sms": 68, "cores_cuda": 8704, "cores_tensor": 272, "register_size": 17825792, "cache_l1": 8912896, "cache_l2": 5242880, "vram": 10737418240, "membw": 816043786240, "fp32_general": 29800000000000.0, "fp16": 238000000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": true, "has_fp8": false, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": true}, {"s": "3080ti", "name_full": "???", "citation": "https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf#page=32", "tdp": 350, "sms": 80, "cores_cuda": 10240, "cores_tensor": 320, "register_size": 20971520, "cache_l1": 10485760, "cache_l2": 6291456, "vram": 12884901888, "membw": 979252543488, "fp32_general": 34100000000000.0, "fp16": 136400000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": true, "has_fp8": false, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": true}, {"s": "3090", "name_full": "???", "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf#page=44", "tdp": 350, "sms": 82, "cores_cuda": 10496, "cores_tensor": 328, "register_size": 21495808, "cache_l1": 10747904, "cache_l2": 6291456, "vram": 25769803776, "membw": 1005022347264, "fp32_general": 35600000000000.0, "fp16": 142000000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": true, "has_fp8": false, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": true}, {"s": "3090ti", "name_full": "???", "citation": "https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf#page=29", "tdp": 450, "sms": 84, "cores_cuda": 10752, "cores_tensor": 336, "register_size": 22020096, "cache_l1": 11010048, "cache_l2": 6291456, "vram": 25769803776, "membw": 1082331758592, "fp32_general": 40000000000000.0, "fp16": 160000000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": true, "has_fp8": false, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": true}, {"s": "A4000", "name_full": "???", "citation": "https://www.nvidia.com/content/dam/en-zz/Solutions/gtcs21/rtx-a4000/nvidia-rtx-a4000-datasheet.pdf", "tdp": 140, "sms": 48, "cores_cuda": 6144, "cores_tensor": 192, "register_size": null, "cache_l1": 8388608, "cache_l2": 4194304, "vram": 17179869184, "membw": 481036337152, "fp32_general": 19200000000000.0, "fp16": 153400000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": true, "has_fp8": false, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": false}, {"s": "A4500", "name_full": "???", "citation": "https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx/nvidia-rtx-a4500-datasheet.pdf", "tdp": 200, "sms": 56, "cores_cuda": 7168, "cores_tensor": 224, "register_size": null, "cache_l1": 8388608, "cache_l2": 6291456, "vram": 21474836480, "membw": 687194767360, "fp32_general": 23700000000000.0, "fp16": 189200000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": true, "has_fp8": false, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": false}, {"s": "A5000", "name_full": "???", "citation": "https://pnypartners.com/wp-content/uploads/nvidia-rtx-a5000-datasheet.pdf", "tdp": 230, "sms": 64, "cores_cuda": 8192, "cores_tensor": 256, "register_size": null, "cache_l1": 8388608, "cache_l2": 6291456, "vram": 25769803776, "membw": 824633720832, "fp32_general": 27800000000000.0, "fp16": 222200000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": true, "has_fp8": false, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": false}, {"s": "A6000", "name_full": "???", "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf#page=15", "tdp": 300, "sms": 84, "cores_cuda": 10752, "cores_tensor": 336, "register_size": 22020096, "cache_l1": 11010048, "cache_l2": 6291456, "vram": 51539607552, "membw": 824633720832, "fp32_general": 38700000000000.0, "fp16": 154800000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": true, "has_fp8": false, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": false}, {"s": "A40", "name_full": "???", "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf#page=15", "tdp": 300, "sms": 84, "cores_cuda": 10752, "cores_tensor": 336, "register_size": 22020096, "cache_l1": 11010048, "cache_l2": 6291456, "vram": 51539607552, "membw": 747324309504, "fp32_general": 37400000000000.0, "fp16": 149700000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": true, "has_fp8": false, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": false}, {"s": "A100", "name_full": "???", "citation": "https://resources.nvidia.com/en-us-genomics-ep/ampere-architecture-white-paper?xs=169656#page=1", "tdp": 400, "sms": 108, "cores_cuda": 6912, "cores_tensor": 432, "register_size": 28311552, "cache_l1": 11010048, "cache_l2": 41943040, "vram": 85899345920, "membw": 1669668536320, "fp32_general": 19500000000000.0, "fp16": 312000000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": true, "has_fp8": false, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": false}, {"s": "L4", "name_full": "???", "citation": "https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf#page=39", "tdp": 72, "sms": 58, "cores_cuda": 7424, "cores_tensor": 232, "register_size": 15204352, "cache_l1": 7602176, "cache_l2": 50331648, "vram": 25769803776, "membw": 322122547200, "fp32_general": 30300000000000.0, "fp16": 121000000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": false, "has_fp8": true, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": false}, {"s": "L40", "name_full": "???", "citation": "https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf#page=36", "tdp": 300, "sms": 142, "cores_cuda": 18176, "cores_tensor": 568, "register_size": 37224448, "cache_l1": 18612224, "cache_l2": 100663296, "vram": 51539607552, "membw": 927712935936, "fp32_general": 90500000000000.0, "fp16": 181000000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": false, "has_fp8": true, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": false}, {"s": "L40S", "name_full": "???", "citation": "https://resources.nvidia.com/en-us-l40s/l40s-datasheet-28413", "tdp": 350, "sms": 142, "cores_cuda": 18176, "cores_tensor": 568, "register_size": 37224448, "cache_l1": 18612224, "cache_l2": 100663296, "vram": 51539607552, "membw": 927712935936, "fp32_general": 90500000000000.0, "fp16": 362050000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": false, "has_fp8": true, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": false}, {"s": "4000A", "name_full": "???", "citation": "https://www.nvidia.com/content/dam/en-zz/Solutions/rtx-4000-sff/proviz-rtx-4000-sff-ada-datasheet-2616456-web.pdf", "tdp": 130, "sms": 48, "cores_cuda": 6144, "cores_tensor": 192, "register_size": 12582912, "cache_l1": 6291456, "cache_l2": 50331648, "vram": 21474836480, "membw": 300647710720, "fp32_general": 19200000000000.0, "fp16": 76700000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": false, "has_fp8": true, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": false}, {"s": "6000A", "name_full": "???", "citation": "https://images.nvidia.com/aem-dam/en-zz/Solutions/technologies/NVIDIA-ADA-GPU-PROVIZ-Architecture-Whitepaper_1.1.pdf#page=28", "tdp": 300, "sms": 142, "cores_cuda": 18176, "cores_tensor": 568, "register_size": 37224448, "cache_l1": 18612224, "cache_l2": 100663296, "vram": 51539607552, "membw": 1030792151040, "fp32_general": 91100000000000.0, "fp16": 364200000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": false, "has_fp8": true, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": false}, {"s": "4090", "name_full": "???", "citation": "https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf#page=13", "tdp": 450, "sms": 128, "cores_cuda": 16384, "cores_tensor": 512, "register_size": 33554432, "cache_l1": 16777216, "cache_l2": 75497472, "vram": 25769803776, "membw": 1082331758592, "fp32_general": 82600000000000.0, "fp16": 330300000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": false, "has_fp8": true, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": true}, {"s": "4080", "name_full": "???", "citation": "https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf#page=13", "tdp": 320, "sms": 76, "cores_cuda": 9728, "cores_tensor": 304, "register_size": 19922944, "cache_l1": 9961472, "cache_l2": 67108864, "vram": 17179869184, "membw": 769657929728, "fp32_general": 48700000000000.0, "fp16": 194900000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": false, "has_fp8": true, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": true}, {"s": "4070ti", "name_full": "???", "citation": "https://www.techpowerup.com/gpu-specs/geforce-rtx-4070-ti.c3950", "tdp": 285, "sms": 60, "cores_cuda": 7680, "cores_tensor": 240, "register_size": 15728640, "cache_l1": 7864320, "cache_l2": 50331648, "vram": 12884901888, "membw": 541380837376, "fp32_general": 40100000000000.0, "fp16": 154800000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": false, "has_fp8": true, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": true}, {"s": "H100-PCIe", "name_full": "???", "citation": "https://resources.nvidia.com/en-us-tensor-core", "tdp": 350, "sms": 114, "cores_cuda": 14592, "cores_tensor": 456, "register_size": 33792, "cache_l1": null, "cache_l2": 52428800, "vram": 85899345920, "membw": 2199023255552, "fp32_general": 66900000000000.0, "fp16": 756500000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": false, "has_fp8": true, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": false}, {"s": "H100-SXM", "name_full": "???", "citation": "https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet", "tdp": 700, "sms": 132, "cores_cuda": 16896, "cores_tensor": 528, "register_size": 33792, "cache_l1": null, "cache_l2": 52428800, "vram": 85899345920, "membw": 3599182594048, "fp32_general": 66900000000000.0, "fp16": 989400000000000.0, "has_bf16": true, "has_tf32": true, "has_int8": true, "has_int4": false, "has_fp8": true, "has_fp6": false, "has_fp4": false, "crippled_fp32acc": false}]